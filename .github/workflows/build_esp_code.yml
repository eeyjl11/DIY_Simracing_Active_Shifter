name: ESP-IDF Build

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

jobs:
  lint:
    name: Lint (clang-tidy via ESP-IDF)
    runs-on: ubuntu-latest
    env:
      IDF_VERSION: v5.5
      # Fixed install locations (cacheable)
      IDF_DIR: ${{ github.workspace }}/.esp/idf/v5.5/esp-idf
      IDF_TOOLS_PATH: ${{ github.workspace }}/.esp/tools
      ESP_IDF_EXPORT_DEBUG: "1"
    steps:
      - uses: actions/checkout@v4

      # Shared cache for esp-idf + tools
      - name: Cache ESP-IDF & tools
        id: idf-cache
        uses: actions/cache@v4
        with:
          path: |
            ~/.espressif
            ${{ env.IDF_TOOLS_PATH }}
            ${{ github.workspace }}/.esp/idf
          key: ${{ runner.os }}-espidf-${{ env.IDF_VERSION }}

      # Clone esp-idf only if cache is cold
      - name: Bootstrap ESP-IDF (first run only)
        if: steps.idf-cache.outputs.cache-hit != 'true'
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p "$(dirname "${IDF_DIR}")"
          if [ -d "${IDF_DIR}/.git" ]; then
            echo "Reusing existing esp-idf repo at ${IDF_DIR}"
            git -C "${IDF_DIR}" remote set-url origin https://github.com/espressif/esp-idf
            git -C "${IDF_DIR}" fetch --tags --depth 1 origin "${IDF_VERSION}"
            git -C "${IDF_DIR}" checkout -f "refs/tags/${IDF_VERSION}" 2>/dev/null || \
            git -C "${IDF_DIR}" checkout -f "${IDF_VERSION}"
          else
            echo "Fresh clone of esp-idf ${IDF_VERSION} into ${IDF_DIR}"
            rm -rf "${IDF_DIR}" || true
            git clone --depth 1 --branch "${IDF_VERSION}" https://github.com/espressif/esp-idf "${IDF_DIR}"
          fi

      # Ensure tools (incl. Python venv) always exist before export.sh (fast no-op on hits)
      - name: Ensure ESP-IDF tools
        shell: bash
        run: |
          set -euo pipefail
          # Make sure the tools land in your cached path
          echo "IDF_TOOLS_PATH=${IDF_TOOLS_PATH}" >> "$GITHUB_ENV"
          # Install required tools (idempotent)
          python3 "${IDF_DIR}/tools/idf_tools.py" install
          # Ensure the Python virtualenv exists (this was missing)
          python3 "${IDF_DIR}/tools/idf_tools.py" install-python-env
          # For the linter job only, ensure esp-clang is present (skip in build job)
          if [[ "${GITHUB_JOB}" == "lint" ]]; then
            python3 "${IDF_DIR}/tools/idf_tools.py" install esp-clang
          fi

      - name: Export & verify
        shell: bash
        run: |
          set -euo pipefail
          # Ensure both paths are set for export.sh
          echo "IDF_PATH=${IDF_DIR}" >> "$GITHUB_ENV"
          echo "IDF_TOOLS_PATH=${IDF_TOOLS_PATH}" >> "$GITHUB_ENV"
          . "${IDF_DIR}/export.sh"
          idf.py --version

      - name: Run clang-tidy (project files only)
        shell: bash
        env:
          IDF_TOOLCHAIN: clang
        continue-on-error: ${{ github.event_name == 'push' }}
        run: |
          set -euo pipefail
          . "${IDF_DIR}/export.sh"
          idf.py set-target esp32s3
      
          # Collect only your sources (adjust patterns if you add more folders/extensions)
          mapfile -t FILES < <(git ls-files \
            'main/**/*.[ch]' 'main/**/*.[ch]pp' \
            'components/**/*.[ch]' 'components/**/*.[ch]pp' || true)
      
          if [ "${#FILES[@]}" -eq 0 ]; then
            echo "No C/C++ project files found under main/ or components/. Skipping clang-tidy."
            exit 0
          fi
      
          # Limit diagnostics to project headers; pass clang-tidy args after '--'
          idf.py clang-check "${FILES[@]}" -- \
            -quiet \
            -header-filter='(^|/)(main|components)/'
      
          # Optional HTML report (if you still want it)
          python -m pip install codereport || true
          idf.py clang-html-report || true

  build:
    name: Build (gated by lint on PRs)
    runs-on: ubuntu-latest
    needs: lint
    env:
      IDF_VERSION: v5.5
      IDF_DIR: ${{ github.workspace }}/.esp/idf/v5.5/esp-idf
      IDF_TOOLS_PATH: ${{ github.workspace }}/.esp/tools
      ESP_IDF_EXPORT_DEBUG: "1"
    steps:
      - uses: actions/checkout@v4

      - name: Restore ESP-IDF & tools cache
        id: idf-cache
        uses: actions/cache@v4
        with:
          path: |
            ~/.espressif
            ${{ env.IDF_TOOLS_PATH }}
            ${{ github.workspace }}/.esp/idf
          key: ${{ runner.os }}-espidf-${{ env.IDF_VERSION }}

      - name: Bootstrap ESP-IDF (first run only)
        if: steps.idf-cache.outputs.cache-hit != 'true'
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p "$(dirname "${IDF_DIR}")"
          if [ -d "${IDF_DIR}/.git" ]; then
            echo "Reusing existing esp-idf repo at ${IDF_DIR}"
            git -C "${IDF_DIR}" remote set-url origin https://github.com/espressif/esp-idf
            git -C "${IDF_DIR}" fetch --tags --depth 1 origin "${IDF_VERSION}"
            git -C "${IDF_DIR}" checkout -f "refs/tags/${IDF_VERSION}" 2>/dev/null || \
            git -C "${IDF_DIR}" checkout -f "${IDF_VERSION}"
          else
            echo "Fresh clone of esp-idf ${IDF_VERSION} into ${IDF_DIR}"
            rm -rf "${IDF_DIR}" || true
            git clone --depth 1 --branch "${IDF_VERSION}" https://github.com/espressif/esp-idf "${IDF_DIR}"
          fi

      - name: Ensure ESP-IDF tools
        shell: bash
        run: |
          set -euo pipefail
          # Make sure the tools land in your cached path
          echo "IDF_TOOLS_PATH=${IDF_TOOLS_PATH}" >> "$GITHUB_ENV"
          # Install required tools (idempotent)
          python3 "${IDF_DIR}/tools/idf_tools.py" install
          # Ensure the Python virtualenv exists (this was missing)
          python3 "${IDF_DIR}/tools/idf_tools.py" install-python-env
          # For the linter job only, ensure esp-clang is present (skip in build job)
          if [[ "${GITHUB_JOB}" == "lint" ]]; then
            python3 "${IDF_DIR}/tools/idf_tools.py" install esp-clang
          fi

      - name: Export & verify
        shell: bash
        run: |
          set -euo pipefail
          # Ensure both paths are set for export.sh
          echo "IDF_PATH=${IDF_DIR}" >> "$GITHUB_ENV"
          echo "IDF_TOOLS_PATH=${IDF_TOOLS_PATH}" >> "$GITHUB_ENV"
          . "${IDF_DIR}/export.sh"
          idf.py --version

      - name: Build
        shell: bash
        # Non-blocking on pushes, blocking on PRs
        continue-on-error: ${{ github.event_name == 'push' }}
        run: |
          set -euo pipefail
          . "${IDF_DIR}/export.sh"
          idf.py set-target esp32s3
          idf.py build

      # Emit size reports for the memcheck job
      - name: Generate size reports
        if: always()
        shell: bash
        run: |
          set -euo pipefail
          . "${IDF_DIR}/export.sh"
          idf.py size --format text --output-file build/size.txt
          idf.py size-components --format csv --output-file build/size-components.csv

      # In the Build job → "Upload size artifacts" step, add the CSV:
      - name: Upload size artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: idf-size
          path: |
            build/size.txt
            build/size-components.csv
            build/partition_table/partition-table.csv    # ← add this
            build/*.map


  memcheck:
    name: Memory Size Check
    runs-on: ubuntu-latest
    needs: build
    continue-on-error: ${{ github.event_name == 'push' }}
    env:
      # Optional hard limits; leave blank to skip
      MAX_IRAM: "16384"     # fail if IRAM goes above this (set 16383 if you want 1B headroom)
      MAX_DIRAM: ""         # e.g. 341760
      # If you set MAX_IMAGE_SIZE, it overrides auto-detected partition size
      MAX_IMAGE_SIZE: ""
      # Soft warnings when near total (percent of reported total)
      WARN_PCT_IRAM: "100"
      WARN_PCT_DIRAM: "90"
    steps:
      - uses: actions/checkout@v4

      - name: Download size artifacts
        uses: actions/download-artifact@v4
        with:
          name: idf-size
          path: build

      - name: Verify memory sizes against thresholds
        shell: bash
        run: |
          set -euo pipefail
          python3 - << 'PY'
          import os, re, sys, csv, pathlib
          size_txt = pathlib.Path('build/size.txt')
          part_csv = pathlib.Path('build/partition_table/partition-table.csv')

          if not size_txt.exists():
              print("ERROR: build/size.txt not found.")
              sys.exit(2)

          # ---- Parse idf.py size table (IRAM, DIRAM, Flash Code/Data, Total image size) ----
          text = size_txt.read_text(encoding='utf-8', errors='ignore').splitlines()
          def first_num(s):
              m = re.search(r'(\d+)', s); return int(m.group(1)) if m else None
          rows = {}
          in_rows = False
          for ln in text:
              if ln.startswith('┡') or ln.startswith('├'): in_rows = True; continue
              if not in_rows: continue
              if ln.startswith('└'): break
              if not ln.startswith('│'): continue
              cols = [c.strip() for c in ln.split('│')]
              if len(cols) < 6: continue
              label = cols[1]
              if label.startswith('.') or label.startswith('    .'): continue
              used = cols[2].replace(',',''); total = cols[5].replace(',','')
              try: u = int(used)
              except: u = None
              try: t = int(total)
              except: t = None
              rows[label] = {'used': u, 'total': t}

          total_image = None
          for ln in text:
              m = re.search(r'Total image size:\s*(\d+)\s*bytes', ln)
              if m: total_image = int(m.group(1)); break

          # ---- Read app partition size from CSV (factory/ota_x) ----
          def parse_bytes(s):
              s = s.strip().upper().rstrip('B')
              if s.endswith('K'): return int(float(s[:-1]) * 1024)
              if s.endswith('M'): return int(float(s[:-1]) * 1024 * 1024)
              if s.startswith('0X'): return int(s, 16)
              return int(s)

          app_slots = []
          if part_csv.exists():
              with part_csv.open(newline='', encoding='utf-8', errors='ignore') as f:
                  for row in csv.reader(f):
                      if not row or row[0].startswith('#'): continue
                      # Name,Type,SubType,Offset,Size,Flags
                      if len(row) >= 5 and row[1].strip() == 'app':
                          name = row[0].strip()
                          try:
                              size = parse_bytes(row[4])
                              app_slots.append((name, size))
                          except Exception:
                              pass

          # Choose the slot we likely built for: prefer 'factory', else largest app slot
          app_limit = None
          chosen = None
          if app_slots:
              for name, size in app_slots:
                  if name == 'factory':
                      chosen = (name, size); break
              if not chosen:
                  chosen = max(app_slots, key=lambda x: x[1])
              app_limit = chosen[1]

          # Allow override via env
          env_limit = os.getenv('MAX_IMAGE_SIZE') or ''
          if env_limit.strip():
              try: app_limit = int(env_limit)
              except: print(f"::warning ::Invalid MAX_IMAGE_SIZE '{env_limit}', ignoring.")

          # ---- Evaluate and report ----
          failures = []
          warnings = []
          out = []

          def check(label, key):
              row = rows.get(key)
              if not row or row['used'] is None: return
              used = row['used']; total = row['total']
              # hard limit
              lim_s = os.getenv(f"MAX_{label.replace(' ','_').upper()}", '')
              if lim_s:
                  try:
                      lim = int(lim_s)
                      if used > lim: failures.append(f"{key} {used}>{lim}")
                  except: warnings.append(f"{key}: invalid limit '{lim_s}'")
              # warn pct if total available
              wp = float(os.getenv(f"WARN_PCT_{label.replace(' ','_').upper()}", '0') or 0)
              if total and wp>0 and used/total*100 >= wp:
                  warnings.append(f"{key} at {used}/{total} (>= {wp}%)")
              out.append(f"{key}: {used}" + (f" / {total}" if total else "") + " bytes")

          check('IRAM', 'IRAM')
          check('DIRAM', 'DIRAM')
          check('FLASH_CODE', 'Flash Code')
          check('FLASH_DATA', 'Flash Data')

          if total_image is not None:
              out.append(f"Total image size: {total_image} bytes")
              if app_limit:
                  out.append(f"App partition limit: {app_limit} bytes")
                  if total_image > app_limit:
                      failures.append(f"Total image size {total_image}>{app_limit}")
              else:
                  warnings.append("App partition size not found; set MAX_IMAGE_SIZE to enforce a limit.")

          print("\n".join(out))
          for w in warnings: print(f"::warning ::{w}")
          pathlib.Path('memcheck-summary.txt').write_text("\n".join(out) + ("\nFAILED: " + ", ".join(failures) if failures else "") + "\n", encoding='utf-8')
          if failures:
              print("ERROR: Memory thresholds exceeded:", ", ".join(failures))
              sys.exit(1)
          PY

      - name: Upload memcheck summary
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: memcheck-summary
          path: memcheck-summary.txt

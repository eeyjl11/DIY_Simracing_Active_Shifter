name: ESP-IDF Build

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

jobs:
  lint:
    name: Lint (clang-tidy via ESP-IDF)
    runs-on: ubuntu-latest
    env:
      IDF_VERSION: v5.5
      # Fixed install locations (cacheable)
      IDF_DIR: ${{ github.workspace }}/.esp/idf/v5.5/esp-idf
      IDF_TOOLS_PATH: ${{ github.workspace }}/.esp/tools
      ESP_IDF_EXPORT_DEBUG: "1"
    steps:
      - uses: actions/checkout@v4

      # Shared cache for esp-idf + tools
      - name: Cache ESP-IDF & tools
        id: idf-cache
        uses: actions/cache@v4
        with:
          path: |
            ~/.espressif
            ${{ env.IDF_TOOLS_PATH }}
            ${{ github.workspace }}/.esp/idf
          key: ${{ runner.os }}-espidf-${{ env.IDF_VERSION }}

      # Clone esp-idf only if cache is cold
      - name: Bootstrap ESP-IDF (first run only)
        if: steps.idf-cache.outputs.cache-hit != 'true'
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p "$(dirname "${IDF_DIR}")"
          if [ -d "${IDF_DIR}/.git" ]; then
            echo "Reusing existing esp-idf repo at ${IDF_DIR}"
            git -C "${IDF_DIR}" remote set-url origin https://github.com/espressif/esp-idf
            git -C "${IDF_DIR}" fetch --tags --depth 1 origin "${IDF_VERSION}"
            git -C "${IDF_DIR}" checkout -f "refs/tags/${IDF_VERSION}" 2>/dev/null || \
            git -C "${IDF_DIR}" checkout -f "${IDF_VERSION}"
          else
            echo "Fresh clone of esp-idf ${IDF_VERSION} into ${IDF_DIR}"
            rm -rf "${IDF_DIR}" || true
            git clone --depth 1 --branch "${IDF_VERSION}" https://github.com/espressif/esp-idf "${IDF_DIR}"
          fi

      # Ensure tools (incl. Python venv) always exist before export.sh (fast no-op on hits)
      - name: Ensure ESP-IDF tools
        shell: bash
        run: |
          set -euo pipefail
          # Make sure the tools land in your cached path
          echo "IDF_TOOLS_PATH=${IDF_TOOLS_PATH}" >> "$GITHUB_ENV"
          # Install required tools (idempotent)
          python3 "${IDF_DIR}/tools/idf_tools.py" install
          # Ensure the Python virtualenv exists (this was missing)
          python3 "${IDF_DIR}/tools/idf_tools.py" install-python-env
          # For the linter job only, ensure esp-clang is present (skip in build job)
          if [[ "${GITHUB_JOB}" == "lint" ]]; then
            python3 "${IDF_DIR}/tools/idf_tools.py" install esp-clang
          fi

      - name: Export & verify
        shell: bash
        run: |
          set -euo pipefail
          # Ensure both paths are set for export.sh
          echo "IDF_PATH=${IDF_DIR}" >> "$GITHUB_ENV"
          echo "IDF_TOOLS_PATH=${IDF_TOOLS_PATH}" >> "$GITHUB_ENV"
          . "${IDF_DIR}/export.sh"
          idf.py --version

      - name: Run clang-tidy (project files only)
        shell: bash
        env:
          IDF_TOOLCHAIN: clang
        continue-on-error: ${{ github.event_name == 'push' }}
        run: |
          set -euo pipefail
          . "${IDF_DIR}/export.sh"
          idf.py set-target esp32s3
      
          # Collect only your sources (adjust patterns if you add more folders/extensions)
          mapfile -t FILES < <(git ls-files \
            'main/**/*.[ch]' 'main/**/*.[ch]pp' \
            'components/**/*.[ch]' 'components/**/*.[ch]pp' || true)
      
          if [ "${#FILES[@]}" -eq 0 ]; then
            echo "No C/C++ project files found under main/ or components/. Skipping clang-tidy."
            exit 0
          fi
      
          # Limit diagnostics to project headers; pass clang-tidy args after '--'
          idf.py clang-check "${FILES[@]}" -- \
            -quiet \
            -header-filter='(^|/)(main|components)/'
      
          # Optional HTML report (if you still want it)
          python -m pip install codereport || true
          idf.py clang-html-report || true

  build:
    name: Build (gated by lint on PRs)
    runs-on: ubuntu-latest
    needs: lint
    env:
      IDF_VERSION: v5.5
      IDF_DIR: ${{ github.workspace }}/.esp/idf/v5.5/esp-idf
      IDF_TOOLS_PATH: ${{ github.workspace }}/.esp/tools
      ESP_IDF_EXPORT_DEBUG: "1"
    steps:
      - uses: actions/checkout@v4

      - name: Restore ESP-IDF & tools cache
        id: idf-cache
        uses: actions/cache@v4
        with:
          path: |
            ~/.espressif
            ${{ env.IDF_TOOLS_PATH }}
            ${{ github.workspace }}/.esp/idf
          key: ${{ runner.os }}-espidf-${{ env.IDF_VERSION }}

      - name: Bootstrap ESP-IDF (first run only)
        if: steps.idf-cache.outputs.cache-hit != 'true'
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p "$(dirname "${IDF_DIR}")"
          if [ -d "${IDF_DIR}/.git" ]; then
            echo "Reusing existing esp-idf repo at ${IDF_DIR}"
            git -C "${IDF_DIR}" remote set-url origin https://github.com/espressif/esp-idf
            git -C "${IDF_DIR}" fetch --tags --depth 1 origin "${IDF_VERSION}"
            git -C "${IDF_DIR}" checkout -f "refs/tags/${IDF_VERSION}" 2>/dev/null || \
            git -C "${IDF_DIR}" checkout -f "${IDF_VERSION}"
          else
            echo "Fresh clone of esp-idf ${IDF_VERSION} into ${IDF_DIR}"
            rm -rf "${IDF_DIR}" || true
            git clone --depth 1 --branch "${IDF_VERSION}" https://github.com/espressif/esp-idf "${IDF_DIR}"
          fi

      - name: Ensure ESP-IDF tools
        shell: bash
        run: |
          set -euo pipefail
          # Make sure the tools land in your cached path
          echo "IDF_TOOLS_PATH=${IDF_TOOLS_PATH}" >> "$GITHUB_ENV"
          # Install required tools (idempotent)
          python3 "${IDF_DIR}/tools/idf_tools.py" install
          # Ensure the Python virtualenv exists (this was missing)
          python3 "${IDF_DIR}/tools/idf_tools.py" install-python-env
          # For the linter job only, ensure esp-clang is present (skip in build job)
          if [[ "${GITHUB_JOB}" == "lint" ]]; then
            python3 "${IDF_DIR}/tools/idf_tools.py" install esp-clang
          fi

      - name: Export & verify
        shell: bash
        run: |
          set -euo pipefail
          # Ensure both paths are set for export.sh
          echo "IDF_PATH=${IDF_DIR}" >> "$GITHUB_ENV"
          echo "IDF_TOOLS_PATH=${IDF_TOOLS_PATH}" >> "$GITHUB_ENV"
          . "${IDF_DIR}/export.sh"
          idf.py --version

      - name: Build
        shell: bash
        # Non-blocking on pushes, blocking on PRs
        continue-on-error: ${{ github.event_name == 'push' }}
        run: |
          set -euo pipefail
          . "${IDF_DIR}/export.sh"
          idf.py set-target esp32s3
          idf.py build

      # Emit size reports for the memcheck job
      - name: Generate size reports
        if: always()
        shell: bash
        run: |
          set -euo pipefail
          . "${IDF_DIR}/export.sh"
          idf.py size --format text --output-file build/size.txt
          idf.py size-components --format csv --output-file build/size-components.csv

      # In the Build job → "Upload size artifacts" step, add the CSV:
      - name: Upload size artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: idf-size
          path: |
            build/size.txt
            build/size-components.csv
            build/partition_table/partition-table.csv    # ← add this
            build/*.map


  memcheck:
    name: Memory Size Check
    runs-on: ubuntu-latest
    needs: build
    continue-on-error: ${{ github.event_name == 'push' }}
    env:
      # Optional hard limits; leave blank to skip
      MAX_IRAM: "16384"
      MAX_DIRAM: "341760"
      # If you set MAX_IMAGE_SIZE, it overrides auto-detected partition size
      MAX_IMAGE_SIZE: ""
      # Soft warnings when near total (percent of reported total)
      WARN_PCT_IRAM: "100"
      WARN_PCT_DIRAM: "90"
      WARN_PCT_IMAGE: "90"
    steps:
      - uses: actions/checkout@v4
  
      - name: Generate size reports
  if: always()
  shell: bash
  run: |
    set -euo pipefail
    . "${IDF_DIR}/export.sh"
    # Create reports even if the build step failed earlier
    mkdir -p build
    # Write the pretty table to a file
    idf.py size > build/size.txt || true
    # Component sizes as CSV
    idf.py size-components --format csv > build/size-components.csv || true

- name: Upload size artifacts
  if: always()
  uses: actions/upload-artifact@v4
  with:
    name: idf-size
    path: |
      build/size.txt
      build/size-components.csv
      build/partition_table/partition-table.csv
      build/*.map
      build/*.bin
      build/*.elf

  
      - name: Verify memory sizes against thresholds
        shell: bash
        run: |
          set -euo pipefail
          cat <<'PY' > memcheck.py
          import os, re, sys, csv, glob, pathlib
      
          size_txt = pathlib.Path('build/size.txt')
          part_csv = pathlib.Path('build/partition_table/partition-table.csv')
      
          if not size_txt.exists():
              print("ERROR: build/size.txt not found.")
              sys.exit(2)
      
          # ---- Parse idf.py size table (IRAM, DIRAM, Flash Code/Data) ----
          text = size_txt.read_text(encoding='utf-8', errors='ignore').splitlines()
          rows = {}
          in_rows = False
          for ln in text:
              if ln.startswith('┡') or ln.startswith('├'):
                  in_rows = True
                  continue
              if not in_rows:
                  continue
              if ln.startswith('└'):
                  break
              if not ln.startswith('│'):
                  continue
              cols = [c.strip() for c in ln.split('│')]
              if len(cols) < 6:
                  continue
              label = cols[1]
              if label.startswith('.') or label.startswith('    .'):
                  continue
              used = cols[2].replace(',', '')
              total = cols[5].replace(',', '')
              try:
                  u = int(used)
              except:
                  u = None
              try:
                  t = int(total)
              except:
                  t = None
              rows[label] = {'used': u, 'total': t}
      
          # Total image size from text (if present)
          total_image = None
          for ln in text:
              m = re.search(r'Total image size:\s*(\d+)\s*bytes', ln)
              if m:
                  total_image = int(m.group(1))
                  break
      
          # Fallback: use the app .bin size (largest .bin excluding bootloader/partition files)
          if total_image is None:
              bins = []
              for p in glob.glob('build/*.bin'):
                  name = pathlib.Path(p).name.lower()
                  if any(tok in name for tok in ('bootloader', 'partition', 'ota_data', 'boot_app')):
                      continue
                  try:
                      bins.append((p, pathlib.Path(p).stat().st_size))
                  except FileNotFoundError:
                      pass
              if bins:
                  bins.sort(key=lambda x: x[1], reverse=True)
                  total_image = bins[0][1]
      
          # ---- Read app partition size from CSV (factory/ota_x) ----
          def parse_bytes(s: str) -> int:
              s = s.strip().upper().rstrip('B')
              if s.endswith('K'): return int(float(s[:-1]) * 1024)
              if s.endswith('M'): return int(float(s[:-1]) * 1024 * 1024)
              if s.startswith('0X'): return int(s, 16)
              return int(s)
      
          app_slots = []
          if part_csv.exists():
              with part_csv.open(newline='', encoding='utf-8', errors='ignore') as f:
                  for row in csv.reader(f):
                      if not row or row[0].startswith('#'):
                          continue
                      if len(row) >= 5 and row[1].strip() == 'app':
                          name = row[0].strip()
                          try:
                              size = parse_bytes(row[4])
                              app_slots.append((name, size))
                          except Exception:
                              pass
      
          app_limit = None
          chosen = None
          if app_slots:
              for name, size in app_slots:
                  if name == 'factory':
                      chosen = (name, size); break
              if not chosen:
                  chosen = max(app_slots, key=lambda x: x[1])
              app_limit = chosen[1]
      
          # Allow override via env
          env_limit = os.getenv('MAX_IMAGE_SIZE') or ''
          if env_limit.strip():
              try:
                  app_limit = int(env_limit)
              except:
                  print(f"::warning ::Invalid MAX_IMAGE_SIZE '{env_limit}', ignoring.")
      
          failures, warnings, out = [], [], []
      
          def check(label_env: str, key_row: str):
              row = rows.get(key_row)
              if not row or row['used'] is None:
                  return
              used = row['used']; total = row['total']
              # hard limit
              lim_s = os.getenv(f"MAX_{label_env}", '')
              if lim_s:
                  try:
                      lim = int(lim_s)
                      if used > lim:
                          failures.append(f"{key_row} {used}>{lim}")
                  except:
                      warnings.append(f"{key_row}: invalid limit '{lim_s}'")
              # warn pct if total available
              wp_s = os.getenv(f"WARN_PCT_{label_env}", '')
              try:
                  wp = float(wp_s) if wp_s else 0.0
              except:
                  wp = 0.0
                  warnings.append(f"{key_row}: invalid WARN_PCT '{wp_s}'")
              if total and wp > 0 and (used / total * 100.0) >= wp:
                  warnings.append(f"{key_row} at {used}/{total} (>= {wp}%)")
              out.append(f"{key_row}: {used}" + (f" / {total}" if total else "") + " bytes")
      
          check('IRAM', 'IRAM')
          check('DIRAM', 'DIRAM')
          check('FLASH_CODE', 'Flash Code')
          check('FLASH_DATA', 'Flash Data')
      
          if total_image is not None:
              out.append(f"Total image size: {total_image} bytes")
              if app_limit:
                  out.append(f"App partition limit: {app_limit} bytes")
                  # hard fail if over limit
                  if total_image > app_limit:
                      failures.append(f"Total image size {total_image}>{app_limit}")
                  # warn percentage for image size
                  wp_img_s = os.getenv('WARN_PCT_IMAGE', '')
                  try:
                      wp_img = float(wp_img_s) if wp_img_s else 0.0
                  except:
                      wp_img = 0.0
                      warnings.append(f"Total image size: invalid WARN_PCT_IMAGE '{wp_img_s}'")
                  if wp_img > 0:
                      pct = 100.0 * total_image / app_limit if app_limit else 0.0
                      if app_limit and pct >= wp_img:
                          warnings.append(f"Total image size at {pct:.2f}% of app partition ({total_image}/{app_limit}) >= {wp_img}%")
              else:
                  warnings.append("App partition size not found; set MAX_IMAGE_SIZE to enforce/warn for image size.")
          else:
              warnings.append("Total image size not found in size.txt or .bin files")
      
          print("\n".join(out))
          for w in warnings:
              print(f"::warning ::{w}")
          pathlib.Path('memcheck-summary.txt').write_text(
              "\n".join(out) + ("\nFAILED: " + ", ".join(failures) if failures else "") + "\n",
              encoding='utf-8'
          )
          if failures:
              print("ERROR: Memory thresholds exceeded:", ", ".join(failures))
              sys.exit(1)
          PY
          python3 memcheck.py
